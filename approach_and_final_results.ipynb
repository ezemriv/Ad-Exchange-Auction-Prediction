{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Project Overview</h1>\n",
    "\n",
    "This project focuses on building a predictive model for determining the probability of winning a bid in an ad-exchange auction. The goal is to surpass the **baseline F1 score of 0.503** using available features in the dataset, which represent various aspects of the auction process.\n",
    "\n",
    "Given hardware limitations, I aimed to use tools and models that are efficient both in terms of memory and speed:\n",
    "\n",
    "- **Polars**: Used for loading and preprocessing the data due to its high performance, especially with larger datasets.\n",
    "- **LightGBM**: Chosen as the model due to its lightweight nature, efficiency, and ability to handle categorical features, making it ideal for this task.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "<pre style=\"font-size:14px;\">\n",
    "AD-EXCHANGE-AUCTION-PREDICTION/\n",
    "├── configs/\n",
    "│   ├── config.yaml\n",
    "├── data/\n",
    "│   ├── test_data.csv\n",
    "│   ├── train_data.csv\n",
    "├── models/\n",
    "│   ├── trained_model.pkl\n",
    "├── notebooks/\n",
    "│   ├── 01_eda_clean.ipynb\n",
    "│   ├── 02_train_infer.ipynb\n",
    "├── results/\n",
    "├── src/\n",
    "│   ├── data_preprocessing/\n",
    "│   │   ├── cleaner.py\n",
    "│   │   ├── feature_engineering.py\n",
    "│   ├── utils/\n",
    "│   │   ├── helpers.py\n",
    "├── approach_and_final_results.ipynb\n",
    "├── environment.yml\n",
    "├── hyperparameter_tuning.py\n",
    "├── LICENSE\n",
    "├── main.py\n",
    "├── README.md\n",
    "</pre>\n",
    "\n",
    "This project structure is designed to ensure modularity and readability. While the `main.py` file contains the core logic for training, cross-validation, and inference, separating data cleaning and feature engineering into the `data_preprocessing/` folder makes it easier to manage and modify those processes independently. Additionally, helper functions in `helpers.py` allow for reusable, clean code, improving the overall maintainability of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Initial Data Processing</h1>\n",
    "\n",
    "\n",
    "The initial data processing focuses on optimizing memory usage, handling missing data, and addressing class imbalance. Key steps include:\n",
    "\n",
    "- **Polars for Data Loading**: Ensuring fast and efficient data loading and preprocessing.\n",
    "  - **Sampling Option**: To load a smaller sample (1M rows) for quicker iteration during development.\n",
    "\n",
    "- **Dropping Non-informative Columns**: Removed columns like `ifa` (too many missing values), `sdk`, `adt`, `dc`, `ssp`, and `os` based on EDA insights.\n",
    "\n",
    "- **Casting Column Types**: \n",
    "  - Floats as `float32` and integers as `int16` to optimize memory usage.\n",
    "  - Categorical columns converted to categorical data types for efficiency.\n",
    "\n",
    "- **Filling Missing Values**: Missing values in categorical columns were filled with `\"unknown\"`.\n",
    "\n",
    "- **Downsampling Due to Class Imbalance**: Addressed the heavily imbalanced target (8.3% positive) by downsampling the negative class. The `neg_ratio` parameter was introduced and will later be optimized using Optuna to improve the F1 score on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Model Training, Cross-Validation, and Inference</h1>\n",
    "\n",
    "The model training process was set up using **LightGBM** with simple initial parameters. Key steps included:\n",
    "\n",
    "- **Training Setup**: LightGBM was selected for its efficiency and performance, with basic model parameters specified in the `config.yaml` file.\n",
    "  \n",
    "- **Cross-Validation Setup**: A K-Fold cross-validation approach was used to evaluate the model's performance on multiple folds. The process outputs F1 scores for both training and validation sets across the folds, providing a robust assessment of the model's generalization capabilities.\n",
    "\n",
    "- **Evaluation on Test Dataset**: After training on the entire dataset, the model was evaluated on the test set. Key outputs include:\n",
    "  - **F1 Score** on the test data to assess model performance.\n",
    "  - **Improvement over Benchmark**: The F1 score is compared against a predefined benchmark from the configuration file, and the percentage improvement is calculated.\n",
    "\n",
    "- **Outputs**:\n",
    "  - **Metrics**: Cross-validation F1 scores, train/test performance, and improvement over the benchmark.\n",
    "  - **Feature Importance**: A plot showing the importance of each feature in the model.\n",
    "  - **Cross-Validation F1 Fold Plots**: A plot showing F1 scores across different validation folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Results\n",
    "\n",
    "### No Downsampling\n",
    "\n",
    "**Mean Train F1 Score**: 0.59354  \n",
    "**Mean Validation F1 Score**: 0.59076  \n",
    "**Test F1 Score**: 0.60681\n",
    "\n",
    "#### **Improvement over Benchmark**: 20.64%\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; width:60%\">\n",
    "    <img src=\"results/01_no_down_no_fe/cross_validation_scores.png\" alt=\"Cross-Validation Plot\" style=\"width:60%;\">\n",
    "    <img src=\"results/01_no_down_no_fe/feature_importances.png\" alt=\"Feature Importance Plot\" style=\"width:60%;\">\n",
    "</div>\n",
    "\n",
    "### Arbitrary NEG_RATIO = 4 Downsampling\n",
    "\n",
    "**Mean Train F1 Score**: 0.75363  \n",
    "**Mean Validation F1 Score**: 0.75002  \n",
    "**Test F1 Score**: 0.60930\n",
    "\n",
    "#### **Improvement over Benchmark**: 21.13%\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; width:60%\">\n",
    "    <img src=\"results/02_neg_ratio4_no_fe/cross_validation_scores.png\" alt=\"Cross-Validation Plot\" style=\"width:60%;\">\n",
    "    <img src=\"results/02_neg_ratio4_no_fe/feature_importances.png\" alt=\"Feature Importance Plot\" style=\"width:60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The following features were engineered to enhance model performance:\n",
    "\n",
    "- **Log Transformation**: Applied log transformation to price-related columns (`flr`, `sellerClearPrice`, `price`) to reduce skewness.\n",
    "  \n",
    "- **Categorical Hour**: Converted the `hour` column into a categorical variable (`hour_cat`) and created hour bands (`hour_band`) for better time-based segmentation (e.g., Morning, Afternoon).\n",
    "\n",
    "- **Language Simplification**: Extracted the general language part from the `lang` column by splitting and retaining the first part, reducing granularity.\n",
    "\n",
    "- **Screen Size**: Combined the width and height of the device's screen into a new categorical `screen_size` feature.\n",
    "\n",
    "- **Low-Frequency Categories**: Used the `replace_less_frequent_polars` function from the helpers module to group low-frequency values in categorical columns (`sdkver`, `lang`, `country`, `region`, `screen_size`) into an \"other\" category.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance After Initial Feature Engineering (NEG_RATIO = 4)\n",
    "\n",
    "**Mean Train F1 Score**: 0.74860  \n",
    "**Mean Validation F1 Score**: 0.74673  \n",
    "**Test F1 Score**: 0.60729  \n",
    "\n",
    "#### **Improvement over Benchmark**: 20.73%\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; width:60%\">\n",
    "    <img src=\"results/03_fe1/cross_validation_scores.png\" alt=\"Cross-Validation Plot\" style=\"width:60%;\">\n",
    "    <img src=\"results/03_fe1/feature_importances.png\" alt=\"Feature Importance Plot\" style=\"width:60%;\">\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Feature Engineering: Price Normalization \n",
    "\n",
    "In addition to the above feature engineering, a second function was implemented to normalize the price columns (`flr`, `sellerClearPrice`, `price`) by the mean and standard deviation for each **Country**, **dsp** and **hour cat**.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance After Price Normalization (NEG_RATIO = 4)\n",
    "\n",
    "**Mean Train F1 Score**: 0.74851  \n",
    "**Mean Validation F1 Score**: 0.74685  \n",
    "**Test F1 Score**: 0.61082  \n",
    "\n",
    "#### **Improvement over Benchmark**: 21.44%\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; width:60%\">\n",
    "    <img src=\"results/04_fe2_group/cross_validation_scores.png\" alt=\"Cross-Validation Plot\" style=\"width:60%;\">\n",
    "    <img src=\"results/04_fe2_group/feature_importances.png\" alt=\"Feature Importance Plot\" style=\"width:60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Hyperparameter Tuning</h1>\n",
    "\n",
    "Based on the cross-validation results, the model was underfitting, likely due to an insufficient setup (e.g., only `n_estimators = 300`). To address this, I used **Optuna** to optimize key hyperparameters related to model complexity while balancing training and inference time. I also tuned the `NEG_RATIO` to improve handling of class imbalance.\n",
    "\n",
    "### Optuna Setup:\n",
    "```python\n",
    "def objective(trial):\n",
    "    NEG_RATIO = trial.suggest_float('neg_ratio', 2, 6)\n",
    "    lgb_params = {\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 64),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 1.0, log=True),\n",
    "    }\n",
    "    # Training code...#\n",
    "    return test_f1\n",
    "```\n",
    "\n",
    "Running 20 trials with Optuna led to a significant improvement in model performance.\n",
    "\n",
    "**Best Trial:**\n",
    "```\n",
    "F1 Score: 0.6195  \n",
    "Parameters: {'neg_ratio': 4.92, 'learning_rate': 0.056, 'max_depth': 6, 'num_leaves': 35, 'subsample': 0.89, 'lambda_l2': 0.00031}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processed successfully.\n",
      "--------------------------------------------------\n",
      "Train data shape: (4747671, 32)\n",
      "Test data shape: (1500000, 32)\n",
      "Number of features: 31\n",
      "--------------------------------------------------\n",
      "% target distribution in train data after downsampling:\n",
      " target\n",
      "0    0.831062\n",
      "1    0.168938\n",
      "Name: proportion, dtype: float64\n",
      "--------------------------------------------------\n",
      "\n",
      " Moving to model training...\n",
      "No oversampling\n",
      "Skipping cross-validation.\n",
      "\n",
      " Training final model...\n",
      "Training time: 96.78 seconds\n",
      "Final model performance on test data:\n",
      "\n",
      "\n",
      " **************************************************\n",
      "F1 on test data: 0.61949\n",
      "Improvement over Benchmark: 23.16%\n",
      "**************************************************\n",
      "Trained model saved at models/trained_model.pkl\n",
      "Metrics saved to results/metrics_20241021_135229.txt\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL MODEL PERFORMANCE After Hyperparameter Tuning\n",
    "\n",
    "**Mean Train F1 Score**: 0.72010  \n",
    "**Mean Validation F1 Score**: 0.71821  \n",
    "**Test F1 Score**: 0.61949  \n",
    "\n",
    "#### **Improvement over Benchmark:** 23.16%\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; width:60%\">\n",
    "    <img src=\"results/05_tuned/cross_validation_scores.png\" alt=\"Cross-Validation Plot\" style=\"width:60%;\">\n",
    "    <img src=\"results/05_tuned/feature_importances.png\" alt=\"Feature Importance Plot\" style=\"width:60%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad_exchange_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
